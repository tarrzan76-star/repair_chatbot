# -*- coding: utf-8 -*-
"""
Streamlit ì•±: ì·¨ê¸‰ ì •ë¹„ì§€ì¹¨ì„œ ì±—ë´‡ (PDF + OCR)
- input_csv í´ë”ì— PDF ì§€ì¹¨ì„œë¥¼ ë„£ìœ¼ë©´ OCRë¡œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•´ ë²¡í„°ìŠ¤í† ì–´ë¥¼ êµ¬ì¶•í•˜ê³  ì§ˆë¬¸ì‘ë‹µì„ ì œê³µí•©ë‹ˆë‹¤.
- OCR: PyMuPDFë¡œ í˜ì´ì§€ ë Œë”ë§ â†’ EasyOCR(ko+en) â†’ ë³¸ë¬¸ í…ìŠ¤íŠ¸ì™€ ë³‘í•©
- LLM: Google Gemini 1.5 Pro (langchain-google-genai)

í•„ìš” íŒ¨í‚¤ì§€(ì˜ˆì‹œ)
-----------------
# CPU í™˜ê²½ ê¸°ì¤€
pip install streamlit pandas loguru pillow numpy
pip install easyocr pymupdf
pip install langchain langchain-community langchain-google-genai faiss-cpu sentence-transformers

Secrets
-------
[â‹¯] â†’ Edit secrets ì— GEMINI_API_KEY ë¥¼ ë“±ë¡í•˜ì„¸ìš”.

í´ë” êµ¬ì¡°
---------
project/
  rechatbot_manual_ocr.py
  input_csv/
    ì¥ë¹„ì •ë¹„ì§€ì¹¨ì„œ1.pdf
    ì¥ë¹„ì •ë¹„ì§€ì¹¨ì„œ2.pdf

ì£¼ì˜
----
- EasyOCRëŠ” ìµœì´ˆ ì‹¤í–‰ ì‹œ ko/en ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤(ì¸í„°ë„· í•„ìš”). íì‡„ë§ì´ë©´ ì‚¬ì „ ë°°í¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.
- PDFì˜ í…ìŠ¤íŠ¸ê°€ ì¶”ì¶œ ê°€ëŠ¥í•œ í˜ì´ì§€ëŠ” OCRì„ ìƒëµí•˜ê³ , ì´ë¯¸ì§€Â·ë„í‘œ ì¤‘ì‹¬ í˜ì´ì§€ëŠ” ìë™ìœ¼ë¡œ OCR í•©ë‹ˆë‹¤.
- ëŒ€ìš©ëŸ‰ PDFëŠ” ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ìºì‹œì™€ ì €ì¥ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""
from __future__ import annotations
import os
import re
import io
from pathlib import Path
from typing import List, Dict, Tuple

import numpy as np
import pandas as pd
from PIL import Image
from loguru import logger

import streamlit as st

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ê¸°ë³¸ ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(
    page_title="ì·¨ê¸‰ ì •ë¹„ì§€ì¹¨ì„œ ì±—ë´‡ (PDF+OCR)",
    layout="wide",
    initial_sidebar_state="expanded",
)

BASE_DIR = Path(__file__).parent
DATA_DIR = BASE_DIR / "input_csv"  # ê¸°ì¡´ í´ë” ì¬í™œìš©
DATA_DIR.mkdir(exist_ok=True)
INDEX_DIR = BASE_DIR / ".manual_index"
INDEX_DIR.mkdir(exist_ok=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìœ í‹¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _clean_text(t: str) -> str:
    t = re.sub(r"[\r\t\f]", " ", t)
    t = re.sub(r"\u200b|\xa0", " ", t)
    t = re.sub(r" +", " ", t).strip()
    return t

@st.cache_resource(show_spinner=False)
def _easyocr_reader():
    import easyocr
    # í•œêµ­ì–´/ì˜ì–´ ë™ì‹œ ì¸ì‹
    return easyocr.Reader(['ko', 'en'], gpu=False)  # GPU í™˜ê²½ì´ë©´ gpu=True ê¶Œì¥

@st.cache_data(show_spinner=False)
def list_pdf_files() -> List[Path]:
    return sorted([p for p in DATA_DIR.glob('*.pdf') if p.is_file()])

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# PDF â†’ í…ìŠ¤íŠ¸ (PyMuPDF + EasyOCR)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _page_to_image_pix(doc, page_index: int, zoom: float = 2.0) -> Image.Image:
    import fitz  # PyMuPDF
    page = doc.load_page(page_index)
    mat = fitz.Matrix(zoom, zoom)
    pix = page.get_pixmap(matrix=mat, alpha=False)
    img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
    return img


def _page_text_density(text: str, area: Tuple[int, int]) -> float:
    # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ë°€ë„ íœ´ë¦¬ìŠ¤í‹±: ê¸€ììˆ˜ / ë©´ì (ë°±ë§Œ px ë‹¨ìœ„)
    w, h = area
    px_m = max(1, (w * h) / 1_000_000)
    return len(text.strip()) / px_m


def extract_pdf_with_ocr(path: Path, force_ocr: bool = False,
                         density_threshold: float = 800.0) -> List[Dict]:
    """
    ê° í˜ì´ì§€ë³„ë¡œ (page, text, meta) ë°˜í™˜
    - ìš°ì„  PyMuPDFì˜ page.get_text("text")ë¥¼ ì‚¬ìš©
    - í…ìŠ¤íŠ¸ ë°€ë„ê°€ ë‚®ê±°ë‚˜(force_ocr), ê¸€ì ìˆ˜ê°€ ë§¤ìš° ì ìœ¼ë©´ EasyOCR ì‹¤í–‰
    - OCR í…ìŠ¤íŠ¸ì™€ ì¶”ì¶œ í…ìŠ¤íŠ¸ë¥¼ ë³‘í•©(ì¤‘ë³µ ì œê±°)
    """
    import fitz  # PyMuPDF

    results: List[Dict] = []
    with fitz.open(path) as doc:
        reader = None
        for i in range(len(doc)):
            page = doc.load_page(i)
            raw = page.get_text("text") or ""
            raw = _clean_text(raw)

            # í…ìŠ¤íŠ¸ ë°€ë„ íŒë‹¨
            w, h = page.rect.width, page.rect.height
            density = _page_text_density(raw, (int(w), int(h)))

            ocr_text = ""
            if force_ocr or len(raw) < 40 or density < density_threshold:
                # OCR ìˆ˜í–‰
                if reader is None:
                    reader = _easyocr_reader()
                img = _page_to_image_pix(doc, i, zoom=2.0)
                np_img = np.array(img)
                ocr_result = reader.readtext(np_img, detail=0, paragraph=True)
                ocr_text = _clean_text("\n".join(ocr_result))

            # ë³‘í•© (ê°„ë‹¨ ì¤‘ë³µ ì œê±°)
            merged = raw
            if ocr_text:
                if raw and ocr_text and ocr_text not in raw:
                    merged = (raw + "\n" + ocr_text).strip()
                elif not raw:
                    merged = ocr_text

            results.append({
                "source": str(path.name),
                "page": i + 1,
                "text": merged,
            })
    return results


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì„ë² ë”© & ë²¡í„°ìŠ¤í† ì–´
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource(show_spinner=False)
def build_embeddings():
    from langchain_community.embeddings import HuggingFaceEmbeddings
    return HuggingFaceEmbeddings(
        model_name="jhgan/ko-sroberta-multitask",
        model_kwargs={"device": "cpu"},
        encode_kwargs={"normalize_embeddings": True},
    )


def _tiktoken_len(text: str) -> int:
    try:
        import tiktoken
        tok = tiktoken.get_encoding("cl100k_base")
        return len(tok.encode(text))
    except Exception:
        return len(text)


def split_to_docs(rows: List[Dict], chunk_size: int = 900, chunk_overlap: int = 120):
    from langchain_core.documents import Document
    from langchain.text_splitter import RecursiveCharacterTextSplitter

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=_tiktoken_len,
    )

    lang_docs = []
    for r in rows:
        meta = {"source": r["source"], "page": r["page"]}
        for chunk in splitter.split_text(r["text"]):
            lang_docs.append(Document(page_content=chunk, metadata=meta))
    return lang_docs


def build_faiss_index(docs):
    from langchain_community.vectorstores import FAISS
    emb = build_embeddings()
    return FAISS.from_documents(docs, emb)


def save_faiss_index(vs, path: Path):
    vs.save_local(str(path))


def load_faiss_index(path: Path):
    from langchain_community.vectorstores import FAISS
    emb = build_embeddings()
    return FAISS.load_local(str(path), emb, allow_dangerous_deserialization=True)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# LLM ì²´ì¸ (Gemini 1.5 Pro)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource(show_spinner=False)
def get_conversation_chain(vstore, gemini_api_key: str):
    from langchain.chains import ConversationalRetrievalChain
    from langchain_google_genai import ChatGoogleGenerativeAI
    from langchain.memory import ConversationBufferMemory

    llm = ChatGoogleGenerativeAI(
        model="gemini-1.5-pro",
        google_api_key=gemini_api_key,
        temperature=0.0,
    )

    chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        chain_type="stuff",
        retriever=vstore.as_retriever(search_type="mmr", search_kwargs={"k": 4}),
        memory=ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True,
            output_key="answer",
        ),
        get_chat_history=lambda h: h,
        return_source_documents=True,
        verbose=False,
    )
    return chain


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Streamlit UI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.title("ğŸ› ï¸ ì·¨ê¸‰ ì •ë¹„ì§€ì¹¨ì„œ ì±—ë´‡ (PDF + OCR)")
st.caption("PDF ì§€ì¹¨ì„œì˜ í…ìŠ¤íŠ¸/ê·¸ë¦¼ì„ OCRë¡œ ì½ê³ , Geminië¡œ ë¬¸ì„œ Q&Aë¥¼ ì œê³µí•©ë‹ˆë‹¤.")

with st.sidebar:
    st.subheader("ğŸ”§ ì„¤ì •")
    gemini_api_key = st.secrets.get("GEMINI_API_KEY", "")
    if not gemini_api_key:
        st.info("Secretsì— GEMINI_API_KEYë¥¼ ë“±ë¡í•˜ì„¸ìš”. (â‹¯ â†’ Edit secrets)")

    st.markdown("---")
    st.subheader("ğŸ“„ ì§€ì¹¨ì„œ ìŠ¤ìº” & ì¸ë±ìŠ¤")
    pdfs = list_pdf_files()
    st.write(f"ê°ì§€ëœ PDF: **{len(pdfs)}**ê±´")
    st.write("- " + "\n- ".join([p.name for p in pdfs]) if pdfs else "input_csv í´ë”ì— PDFë¥¼ ë„£ìœ¼ì„¸ìš”.")

    force_ocr = st.checkbox("ëª¨ë“  í˜ì´ì§€ OCR ê°•ì œ", value=False)
    density_th = st.slider("OCR ì „í™˜ ì„ê³„(í…ìŠ¤íŠ¸ ë°€ë„)", 100.0, 2000.0, 800.0, 50.0,
                           help="ë‚®ì„ìˆ˜ë¡ ë” ë§ì€ í˜ì´ì§€ì—ì„œ PyMuPDF í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    do_build = st.button("ğŸ“š ì¸ë±ìŠ¤ ìƒì„±/ê°±ì‹ ")

# ì„¸ì…˜ ìƒíƒœ
if "qa_chain" not in st.session_state:
    st.session_state.qa_chain = None
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! ì¢Œì¸¡ì—ì„œ PDF ì¸ë±ìŠ¤ë¥¼ ë¨¼ì € ìƒì„±í•œ í›„, ì•„ë˜ ì±„íŒ…ì°½ì— ì§ˆë¬¸í•´ ì£¼ì„¸ìš”."}]

# ì¸ë±ìŠ¤ ìƒì„±
if do_build:
    if not pdfs:
        st.warning("input_csv í´ë”ì— PDFê°€ ì—†ìŠµë‹ˆë‹¤.")
    elif not gemini_api_key:
        st.warning("Gemini API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    else:
        all_rows: List[Dict] = []
        progress = st.progress(0)
        status = st.empty()

        for idx, p in enumerate(pdfs, start=1):
            status.info(f"OCR/ì¶”ì¶œ ì¤‘: {p.name} ({idx}/{len(pdfs)})")
            try:
                rows = extract_pdf_with_ocr(p, force_ocr=force_ocr, density_threshold=density_th)
                all_rows.extend(rows)
            except Exception as e:
                st.error(f"{p.name} ì²˜ë¦¬ ì‹¤íŒ¨: {type(e).__name__}: {e}")
            progress.progress(idx / max(1, len(pdfs)))

        status.info("í…ìŠ¤íŠ¸ ë¶„í•  ë° ë²¡í„° ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
        docs = split_to_docs(all_rows)
        vdb = build_faiss_index(docs)

        save_path = INDEX_DIR / "faiss_manual"
        save_faiss_index(vdb, save_path)
        st.session_state.qa_chain = get_conversation_chain(vdb, gemini_api_key)
        status.success("ì™„ë£Œ! ì¸ë±ìŠ¤ ì €ì¥ ë° QA ì²´ì¸ ì¤€ë¹„ê°€ ëë‚¬ìŠµë‹ˆë‹¤.")
        st.success(f"ì´ ì²­í¬ ìˆ˜: {len(docs):,} ê°œ")

# ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì‹œë„
if st.session_state.qa_chain is None:
    saved = INDEX_DIR / "faiss_manual"
    if (saved.with_suffix(".pkl").exists() or (saved / "index.faiss").exists()):
        try:
            vdb = load_faiss_index(saved)
            st.session_state.qa_chain = get_conversation_chain(vdb, st.secrets.get("GEMINI_API_KEY", ""))
            st.info("ê¸°ì¡´ ì¸ë±ìŠ¤ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
        except Exception:
            pass

# ì±„íŒ… UI
st.markdown("---")
st.subheader("ğŸ’¬ ë¬¸ì„œ Q&A")

for m in st.session_state.messages:
    with st.chat_message(m["role"]):
        st.markdown(m["content"])

user_q = st.chat_input("ì •ë¹„ì§€ì¹¨ì„œì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš” (ì˜ˆ: ë¹„ìƒì œë™ ì ˆì°¨, í”¼ê²¬ì¸ ìš´ì „ ì ˆì°¨ ë“±)")
if user_q:
    if st.session_state.qa_chain is None:
        st.warning("ë¨¼ì € ì¢Œì¸¡ì—ì„œ ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•˜ì„¸ìš”.")
    else:
        st.session_state.messages.append({"role": "user", "content": user_q})
        with st.chat_message("user"):
            st.markdown(user_q)
        with st.chat_message("assistant"):
            with st.spinner("ê²€ìƒ‰ ë° ì‘ë‹µ ìƒì„± ì¤‘..."):
                result = st.session_state.qa_chain({"question": user_q})
                answer = result.get("answer", "")
                srcs = result.get("source_documents", [])
                st.markdown(answer)
                if srcs:
                    with st.expander("ì°¸ê³  ì†ŒìŠ¤"):
                        for i, d in enumerate(srcs[:6], start=1):
                            src = d.metadata.get("source", "unknown")
                            page = d.metadata.get("page", None)
                            meta = f"{src}" + (f" (p.{page})" if isinstance(page, int) else "")
                            st.markdown(f"**{i}.** {meta}")
        st.session_state.messages.append({"role": "assistant", "content": answer})

st.markdown("---")
st.caption(
    "â“˜ íŒ: OCR ì„ê³„ê°’ì„ ë†’ì´ë©´ ê·¸ë¦¼ ë§ì€ í˜ì´ì§€ì—ì„œ OCRì„ ë” ìì£¼ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì§ˆë¬¸ì€ êµ¬ì²´ì ìœ¼ë¡œ ì ì„ìˆ˜ë¡ ì •í™•ë„ê°€ ë†’ì•„ì§‘ë‹ˆë‹¤.")
